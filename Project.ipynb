{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('2021_Green_Taxi_Trip_Data.csv')\n",
    "\n",
    "# Drop irrelevant columns\n",
    "irrelevant_columns = ['RatecodeID', 'store_and_fwd_flag', 'improvement_surcharge',\n",
    "                      'ehail_fee', 'mta_tax', 'extra', 'fare_amount',\n",
    "                      'congestion_surcharge']\n",
    "df = df.drop(irrelevant_columns, axis=1)\n",
    "\n",
    "# Convert relevant columns to appropriate types\n",
    "df['payment_type'] = df['payment_type'].astype(object)\n",
    "df['trip_type'] = df['trip_type'].astype(object)\n",
    "\n",
    "# Handle specific missing values\n",
    "df.loc[(df['tip_amount'] > 0) & (df['payment_type'].isnull()), 'payment_type'] = 1\n",
    "df.loc[df['passenger_count'] > 4, 'payment_type'] = 6\n",
    "df.loc[df['passenger_count'] > 4, 'trip_type'] = 3\n",
    "df.dropna(subset=['VendorID'], inplace=True)\n",
    "\n",
    "# Convert pickup datetime\n",
    "df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "df = df[df['pickup_datetime'].dt.year == 2021]\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns, 'percent_missing': percent_missing})\n",
    "print(missing_value_df)\n",
    "# Extract hour from pickup_datetime\n",
    "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "df['pickup_day'] = df['pickup_datetime'].dt.day\n",
    "df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "\n",
    "# Extract dropoff datetime\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "df['dropoff_hour'] = df['dropoff_datetime'].dt.hour\n",
    "df['dropoff_day'] = df['dropoff_datetime'].dt.day\n",
    "df['dropoff_month'] = df['dropoff_datetime'].dt.month\n",
    "\n",
    "# Drop original datetime columns\n",
    "df.drop(columns=['lpep_pickup_datetime', 'lpep_dropoff_datetime'], inplace=True)\n",
    "\n",
    "# Merge borough information into the main DataFrame\n",
    "borough_df = pd.read_csv('Boroughs.csv')\n",
    "df = df.merge(borough_df[['LocationID', 'Borough']], left_on='PULocationID', right_on='LocationID', how='left')\n",
    "df.rename(columns={'Borough': 'PU_Borough'}, inplace=True)\n",
    "\n",
    "df = df.merge(borough_df[['LocationID', 'Borough']], left_on='DOLocationID', right_on='LocationID', how='left')\n",
    "df.rename(columns={'Borough': 'DO_Borough'}, inplace=True)\n",
    "\n",
    "# One-hot encode boroughs\n",
    "df = pd.get_dummies(df, columns=['PU_Borough'], prefix=['PU'], drop_first=True)\n",
    "df.drop([ 'PULocationID', 'DOLocationID', 'LocationID_x', 'LocationID_y','pickup_datetime', 'dropoff_datetime' ,'DO_Borough'], axis=1, inplace=True)\n",
    "print(df.columns)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "# Handle missing value using regression or classification\n",
    "for column in df.columns:\n",
    "    null_percentage = df[column].isnull().mean()\n",
    "    if 0.05 < null_percentage <= 0.3:\n",
    "        missing_rows = df[df[column].isnull()]\n",
    "        complete_rows = df[df[column].notnull()]\n",
    "\n",
    "        # Ensure you drop datetime columns prior to fitting\n",
    "        features_to_drop = [column]\n",
    "        if complete_rows[column].dtype in ['int64', 'float64']:\n",
    "            model = HistGradientBoostingRegressor()\n",
    "            model.fit(complete_rows.drop(columns=features_to_drop, errors='ignore'), complete_rows[column])\n",
    "            predicted_values = model.predict(missing_rows.drop(columns=features_to_drop, errors='ignore'))\n",
    "            df.loc[df[column].isnull(), column] = predicted_values\n",
    "\n",
    "\n",
    "        else:\n",
    "            model = HistGradientBoostingClassifier()\n",
    "            model.fit(complete_rows.drop(columns=features_to_drop, errors='ignore'), complete_rows[column])\n",
    "            predicted_values = model.predict(missing_rows.drop(columns=features_to_drop, errors='ignore'))\n",
    "            df.loc[df[column].isnull(), column] = predicted_values\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Calculate and display missing values percentage\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns, 'percent_missing': percent_missing})\n",
    "print(missing_value_df)\n",
    "\n",
    "# Extract hour from pickup_datetime\n",
    "df_1=df[:100000]\n",
    "print(df_1.columns)\n",
    "# 1. Trip Type Distribution by Hour  \n",
    "trip_type_hourly_distribution = df_1.groupby(['pickup_hour', 'trip_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the Trip Type Distribution by Hour  \n",
    "plt.figure(figsize=(14, 7))  \n",
    "sns.barplot(data=trip_type_hourly_distribution.reset_index().melt(id_vars=['pickup_hour']),\n",
    "            x='pickup_hour', y='value', hue='trip_type', errorbar=None)\n",
    "plt.title('Trip Type Distribution by Hour of Day')  \n",
    "plt.xlabel('Hour of Day')  \n",
    "plt.ylabel('Number of Trips')  \n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability  \n",
    "plt.legend(title='Trip Type', bbox_to_anchor=(1.05, 1), loc='upper left')  \n",
    "plt.tight_layout()  \n",
    "plt.show()  \n",
    "\n",
    "# 2. Payment Type Distribution by Hour  \n",
    "payment_type_hourly_distribution = df_1.groupby(['pickup_hour', 'payment_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the Payment Type Distribution by Hour  \n",
    "plt.figure(figsize=(14, 7))  \n",
    "sns.barplot(data=payment_type_hourly_distribution.reset_index().melt(id_vars=['pickup_hour']),\n",
    "            x='pickup_hour', y='value', hue='payment_type', ci=None)\n",
    "plt.title('Payment Type Distribution by Hour of Day')  \n",
    "plt.xlabel('Hour of Day')  \n",
    "plt.ylabel('Number of Trips')  \n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability  \n",
    "plt.legend(title='Payment Type', bbox_to_anchor=(1.05, 1), loc='upper left')  \n",
    "plt.tight_layout()  \n",
    "plt.show()  \n",
    "\n",
    "# 3. حذف سفرهایی که نوع آن‌ها dispatch است\n",
    "taxi_df = df_1[df_1['trip_type'] != 3]\n",
    "# بررسی استفاده از تاکسی سبز بر اساس بخش‌های شهری فقط برای تاکسی‌ها\n",
    "green_taxi_usage = taxi_df.groupby('pickup_hour').size()\n",
    "\n",
    "# رسم نمودار\n",
    "plt.figure(figsize=(10, 6))\n",
    "green_taxi_usage.plot(kind='bar', color='green')\n",
    "plt.title('Green Taxi Usage by Hour of Day ')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Green Taxi Trips')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# 4\n",
    "# Grouping by all dummy variables for boroughs and pickup hour\n",
    "df_1['Pickup_Borough'] = df_1.apply(\n",
    "    lambda row: 'Brooklyn' if row['PU_Brooklyn'] == 1 else\n",
    "                'EWR' if row['PU_EWR'] == 1 else\n",
    "                'Manhattan' if row['PU_Manhattan'] == 1 else\n",
    "                'Queens' if row['PU_Queens'] == 1 else\n",
    "                'Staten Island' if row['PU_Staten Island'] == 1 else\n",
    "                'Bronx', axis=1\n",
    ")\n",
    "green_taxi_trend = df_1.groupby([ 'Pickup_Borough','pickup_hour']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting the trend of green taxi usage by boroughs\n",
    "plt.figure(figsize=(14, 7))\n",
    "green_taxi_trend.T.plot(kind='bar', figsize=(14, 7))\n",
    "plt.title('Green Taxi Usage Trend by Borough')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.legend(title='Borough', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "df_1 = df_1.drop('Pickup_Borough', axis=1)\n",
    "\n",
    "##############            بخش دوم\n",
    "# محاسبه ماتریس همبستگی\n",
    "correlation_matrix = df_1.corr()\n",
    "# رسم ماتریس همبستگی به صورت Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Predicting Total Amount')\n",
    "plt.show()\n",
    "\n",
    "# تحلیل: ستون‌هایی که بیشترین همبستگی را با total_amount دارند\n",
    "correlation_with_total = correlation_matrix['total_amount'].sort_values(ascending=False)\n",
    "print(\"Top correlations with total_amount:\")\n",
    "print(correlation_with_total)\n",
    "\n",
    "##### بخش دوم خواسته سوم\n",
    "\n",
    "def backward_feature_selection(data, target_column='total_amount', n_features=5):\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(estimator=model, n_features_to_select=n_features)\n",
    "    rfe.fit(X, y)\n",
    "    selected_features = X.columns[rfe.support_]\n",
    "    return list(selected_features)\n",
    "\n",
    "backward_selected_features = backward_feature_selection(df_1, target_column='total_amount', n_features=5)\n",
    "print(\"Backward Selected Features:\", backward_selected_features)\n",
    "\n",
    "\n",
    "##### بخش دوم خواسته سوم\n",
    "\n",
    "def forward_feature_selection(data, target_column='total_amount', n_features=5):\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    model = LinearRegression()\n",
    "    selected_features = []\n",
    "    remaining_features = list(X.columns)\n",
    "    while len(selected_features) < n_features and remaining_features:\n",
    "        best_score = -np.inf\n",
    "        best_feature = None\n",
    "        for feature in remaining_features:\n",
    "            trial_features = selected_features + [feature]\n",
    "            scores = cross_val_score(model, X[trial_features], y, cv=5, scoring='r2')\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_feature = feature\n",
    "        if best_feature is not None:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "    return selected_features\n",
    "forward_selected_features = forward_feature_selection(df_1, target_column='total_amount', n_features=5)\n",
    "print(\"Forward Selected features:\", forward_selected_features)\n",
    "\n",
    "##### بخش  خواسته سوم\n",
    "\n",
    "def random_forest_feature_selection(data, target_column='total_amount', n_features=5):\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "    selected_features = feature_importances.nlargest(n_features).index.tolist()\n",
    "    return selected_features\n",
    "rf_selected_features = random_forest_feature_selection(df_1, target_column='total_amount', n_features=5)\n",
    "print(\"Random Forest Selected Features:\", rf_selected_features)\n",
    "\n",
    "\n",
    "def chi_square_feature_selection(data, target_column='pct', n_features=5):    \n",
    "    if data[target_column].dtype not in ['int64', 'int32', 'bool', 'uint8']:\n",
    "        data[target_column] = pd.qcut(data[target_column], q=4, labels=False)\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    for column in X.columns:\n",
    "        if X[column].dtype in ['float64', 'int64']:\n",
    "            X[column] = pd.cut(X[column], bins=5, labels=False)\n",
    "        X[column] = X[column].clip(lower=0)\n",
    "    chi_selector = SelectKBest(score_func=chi2, k=n_features)\n",
    "    X_kbest = chi_selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[chi_selector.get_support()]\n",
    "    return selected_features\n",
    "# اجرای chi-square روی داده‌ها\n",
    "chi_selected_features = chi_square_feature_selection(df_1, target_column='total_amount', n_features=5)\n",
    "print(\"Chi-Square Selected Features (Chi-Square):\", list(chi_selected_features))\n",
    "\n",
    "######################### بخش سوم\n",
    "\n",
    "df_1['tip_given'] = (df_1['tip_amount'] > 0).astype(int)\n",
    "X = df_1.drop(columns=['tip_given', 'tip_amount'])  # حذف ستون‌های غیرضروری\n",
    "y = df_1['tip_given']\n",
    "\n",
    "# تقسیم داده‌ها به مجموعه‌های آموزش و تست\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(\"Decision Tree Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "\n",
    "# 2. Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"\\nRandom Forest Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "# 3. XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"\\nXGBoost Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n",
    "#### بخش 4 خواسته 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Function to train and evaluate linear regression model\n",
    "def evaluate_linear_regression(features, X, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[features], y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and evaluation\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return mse, r2\n",
    "\n",
    "# Define target variable and features\n",
    "y = df_1['total_amount']\n",
    "X = df_1.drop(columns=['total_amount'])\n",
    "\n",
    "# Feature subsets from different selection methods\n",
    "feature_selection_methods = {\n",
    "    'Backward': backward_selected_features,\n",
    "    'Forward': forward_selected_features,\n",
    "    'Random Forest': rf_selected_features,\n",
    "    'Chi-Square': list(chi_selected_features)\n",
    "}\n",
    "\n",
    "# Evaluate Linear Regression for each feature subset\n",
    "results = []\n",
    "for method, features in feature_selection_methods.items():\n",
    "    mse, r2 = evaluate_linear_regression(features, X, y)\n",
    "    results.append({'Method': method, 'MSE': mse, 'R^2': r2})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best feature set based on MSE\n",
    "best_method = results_df.loc[results_df['MSE'].idxmin()]\n",
    "best_features = feature_selection_methods[best_method['Method']]\n",
    "\n",
    "print(\"Feature Selection Results:\")\n",
    "print(results_df)\n",
    "print(\"\\nBest Feature Set Based on MSE:\")\n",
    "print(f\"Method: {best_method['Method']}, MSE: {best_method['MSE']}, R^2: {best_method['R^2']}\")\n",
    "\n",
    "# Integrate the best Linear Regression model with Random Forest and XGBoost for comparison\n",
    "# 1. Train and evaluate the best Linear Regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[best_features], y, test_size=0.2, random_state=42)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# 3. XGBoost Regressor\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "xgb_mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "xgb_r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Comparison of models\n",
    "comparison_results = {\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Mean Squared Error': [lr_mse, rf_mse, xgb_mse],\n",
    "    'R^2 Score': [lr_r2, rf_r2, xgb_r2]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "best_model = comparison_df.loc[comparison_df['Mean Squared Error'].idxmin()]\n",
    "\n",
    "# Display the best model based on MSE\n",
    "print(\"\\nBest Model Based on MSE:\")\n",
    "print(f\"Method: {best_model['Model']}, MSE: {best_model['Mean Squared Error']:.4f}, R^2: {best_model['R^2 Score']:.4f}\")\n",
    "# # Assuming you used `rf_selected_features` for training:\n",
    "# X_train_rf = X_train[rf_selected_features]\n",
    "# X_test_rf = X_test[rf_selected_features]\n",
    "#\n",
    "# # Fit the RandomForest model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train_rf, y_train)\n",
    "#\n",
    "# # Now, ensure that the feature importances match the selected features\n",
    "# rf_importances = pd.Series(rf_model.feature_importances_, index=rf_selected_features)\n",
    "#\n",
    "# # Plotting the top 10 important features\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# rf_importances.nlargest(10).plot(kind='bar', title='Random Forest Feature Importances')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "#\n",
    "# Plotting feature importance for tree-based models\n",
    "rf_importances = pd.Series(rf_model.feature_importances_,index=X.columns)\n",
    "xgb_importances = pd.Series(xgb_model.feature_importances_,index=X.columns)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "rf_importances.nlargest(10).plot(kind='bar', title='Random Forest Feature Importances')\n",
    "plt.subplot(1, 2, 2)\n",
    "xgb_importances.nlargest(10).plot(kind='bar', title='XGBoost Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
